# Neural Network from Scratch

This project implements a neural network from scratch using NumPy and fundamental mathematical concepts. The goal is to provide a deep understanding of the inner workings of neural networks without relying on high-level deep learning frameworks.

## Features

- Implementation of feedforward neural networks
- Backpropagation algorithm for training
- Various activation functions (e.g., sigmoid, ReLU, tanh)
- Customizable network architecture
- Basic optimization techniques (e.g., gradient descent, mini-batch gradient descent)

## Requirements

- Python 3.7+
- NumPy

## Installation

1. Clone this repository:

   git clone git@github.com:AryanTele/NeuralNetwork_from_scratch.git
   
3. Install the required dependencies:

   pip install numpy


## Project Structure

- `neural_network.py`: Contains the main NeuralNetwork class
- `activation_functions.py`: Implements various activation functions
- `loss_functions.py`: Defines loss functions for training
- `utils.py`: Utility functions for data preprocessing and visualization
- `examples/`: Directory containing example usage and datasets

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## Acknowledgments

- [NumPy documentation](https://numpy.org/doc/)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen
